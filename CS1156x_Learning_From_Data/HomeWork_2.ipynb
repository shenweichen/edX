{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a computer simulation for flipping 1,000 virtual fair coins. Flip each coin independently 10 times. Focus on 3 coins as follows: c1 is the first coin flipped, crand is a coin chosen randomly from the 1,000, and cmin is the coin which had the minimum frequency of heads (pick the earlier one in case of a tie). Let ν1, νrand, and νmin be the fraction of heads obtained for the 3 respective coins out of the 10 tosses. Run the experiment 100,000 times in order to get a full distribution of ν1, νrand, and νmin (note that crand and cmin will change from run to run).  \n",
    "1. The average value of νmin is closest to:  \n",
    "[a] 0 **[b] 0.01** [c] 0.1 [d] 0.5 [e] 0.67  \n",
    "2. Which coin(s) has a distribution of ν that satisfies the (single-bin) Hoeffding Inequality?  \n",
    "[a] c1 only [b] crand only [c] cmin only **[d] c1 and crand**\n",
    "[e] cmin and cran  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(times):\n",
    "    v_1 = 0\n",
    "    v_rand = 0\n",
    "    v_min = 0\n",
    "    for t in range(0,times):\n",
    "        head_freq = [0]*1000\n",
    "        c_1 = 0\n",
    "        c_rand = random.randint(0,999)\n",
    "        for i in range(0,1000):\n",
    "            for _ in range(0,10):\n",
    "                head_freq[i] += random.randint(0,1)\n",
    "        c_min = head_freq.index(min(head_freq))\n",
    "        v_1 += head_freq[c_1]\n",
    "        v_rand += head_freq[c_rand]\n",
    "        v_min += head_freq[c_min]\n",
    "    return v_1/(times*10.0),v_rand/(times*10.0),v_min/(times*10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_1= 0.499852 v_rand= 0.499968 v_min= 0.037697\n"
     ]
    }
   ],
   "source": [
    "v_1,v_rand,v_min = experiment(100000)\n",
    "print \"v_1=\",v_1,\"v_rand=\",v_rand,\"v_min=\",v_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these problems, we will explore how Linear Regression for classification works. As with the Perceptron Learning Algorithm in Homework # 1, you will create your own target function f and data set D. Take d = 2 so you can visualize the problem, and assume X = [−1, 1] × [−1, 1] with uniform probability of picking each x ∈ X. In each run, choose a random line in the plane as your target function f (do this by taking two random, uniformly distributed points in [−1, 1] × [−1, 1] and taking the line passing through them), where one side of the line maps to +1 and the other maps to −1. Choose the inputs xn of the data set as random points (uniformly in X), and evaluate the target function on each xn to get the corresponding output yn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WLIN = np.array([0,0,0],dtype=np.float)\n",
    "WF = np.array([0,0,-1],dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line(x,w):\n",
    "    return (-w[0]-w[1]*x)/w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw(data,label,):\n",
    "    idx_1 = np.where(label==1)\n",
    "    p1 = plt.scatter(data[idx_1,1],data[idx_1,2],marker='o',color = 'g',label = '1',s = 30)\n",
    "\n",
    "    idx_2 = np.where(label==-1)\n",
    "    p2 = plt.scatter(data[idx_2,1],data[idx_2,2],marker='x',color = 'r',label = '-1',s = 30)\n",
    "\n",
    "    p_WLIN = plt.plot([-1,1],line(np.array([-1,1]),WLIN),'b-',linewidth=2,label=\"WLIN\")\n",
    "    p_WF = plt.plot([-1,1],line(np.array([-1,1]),WF),'r-.',linewidth=2,label=\"WF\")\n",
    "    plt.legend(loc='upper right')# add legend,need to set label first\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slove_LRegress(X,y):\n",
    "    global WLIN\n",
    "    A = np.dot(X.T,X)\n",
    "    pseudo_inverse = np.dot(np.linalg.inv(A),X.T)\n",
    "    WLIN = np.dot(pseudo_inverse,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Linear_Regression(N,noise=False,pic = False):\n",
    "    global WF\n",
    "    data = np.random.uniform(-1,1,(N,2))\n",
    "    data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "    point = np.random.uniform(-1,1,(2,2))\n",
    "    k = (point[0][1] - point[1][1])/(point[0][0] - point[1][0])\n",
    "    b = point[0][1] - k*point[0][0]\n",
    "    WF = np.array([b,k,-1],dtype=np.float)\n",
    "    label = np.dot(data,WF)\n",
    "    label[label>0]=1\n",
    "    label[label<=0]=-1\n",
    "    if noise:\n",
    "        noise_point = random.sample(range(0,N),int(N*0.1))\n",
    "        for i in noise_point:\n",
    "            label[i] =-label[i]\n",
    "    slove_LRegress(data,label)\n",
    "    if pic:\n",
    "        draw(data,label)\n",
    "    error = 0\n",
    "    pred = np.dot(data,WLIN)\n",
    "    pred[pred>0]=1\n",
    "    pred[pred<=0]=-1\n",
    "    error = sum(pred!=label)\n",
    "    return error/float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Take N = 100. Use Linear Regression to find g and evaluate Ein, the fraction of in-sample points which got classified incorrectly. Repeat the experiment 1000 times and take the average (keep the g’s as they will be used again in Problem 6). Which of the following values is closest to the average Ein? (Closest is the option that makes the expression |your answer−given option| closest to 0. Use this definition of closest here and throughout.)  \n",
    "[a] 0 [b] 0.001 **[c] 0.01** [d] 0.1\n",
    "[e] 0.5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_Ein(N,times= 1000,noise = False):\n",
    "    E_in = 0\n",
    "    for _ in range(times):\n",
    "        E_in += Linear_Regression(N,noise,pic = False,)\n",
    "    return E_in/float(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in= 0.04029\n"
     ]
    }
   ],
   "source": [
    "print \"E_in=\",compute_Ein(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Now, generate 1000 fresh points and use them to estimate the out-of-sample error Eout of g that you got in Problem 5 (number of misclassified out-of-sample points / total number of out-of-sample points). Again, run the experiment 1000 times and take the average. Which value is closest to the average Eout?  \n",
    "[a] 0 [b] 0.001 **[c] 0.01** [d] 0.1\n",
    "[e] 0.5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_Eout(N,times = 1000):\n",
    "    E_out = 0\n",
    "    for _ in range(times):\n",
    "        data = np.random.uniform(-1,1,(N,2))\n",
    "        data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "        label = np.dot(data,WF)\n",
    "        label[label>0]=1\n",
    "        label[label<=0]=-1\n",
    "        pred = np.dot(data,WLIN)\n",
    "        pred[pred>0]=1\n",
    "        pred[pred<=0]=-1\n",
    "        error = sum(pred!=label)\n",
    "        E_out += error/float(N)\n",
    "    return E_out/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_out= 0.036851\n"
     ]
    }
   ],
   "source": [
    "print \"E_out=\",compute_Eout(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Now, take N = 10. After finding the weights using Linear Regression, use them as a vector of initial weights for the Perceptron Learning Algorithm. Run PLA until it converges to a final vector of weights that completely separates all the in-sample points. Among the choices below, what is the closest value to the average number of iterations (over 1000 runs) that PLA takes to converge? (When implementing PLA, have the algorithm choose a point randomly from the set of misclassified points at each iteration)  \n",
    "**[a] 1** [b] 15 [c] 300 [d] 5000\n",
    "[e] 10000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PLA(data,label):\n",
    "    WPLA = WLIN\n",
    "    #初始化WPLA\n",
    "    numofdata = data.shape[0]\n",
    "    t = 0\n",
    "    while True:\n",
    "        pred = np.dot(data,WPLA)\n",
    "        pred[pred>0]=1\n",
    "        pred[pred<=0]=-1\n",
    "        error_point = np.where(pred!=label)[0]\n",
    "        if error_point.size == 0:\n",
    "            return t\n",
    "        fix_point = random.sample(error_point,1)[0]#从分类错误的点集中随机选取一个进行修正\n",
    "        t += 1\n",
    "        WPLA += label[fix_point]*data[fix_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_iterations(N,times=1000):\n",
    "    num_iter = 0\n",
    "    for _ in range(times):\n",
    "        data = np.random.uniform(-1, 1, (N, 2))\n",
    "        data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "        #随机生成10个长度为3的数组\n",
    "        label = np.dot(data,WF)\n",
    "        label[label>0]=1\n",
    "        label[label<=0]=-1\n",
    "        num_iter+= PLA(data,label)    \n",
    "    return num_iter/float(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations= 0.35\n"
     ]
    }
   ],
   "source": [
    "print \"num_iterations=\",num_iterations(10,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Nonlinear Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these problems, we again apply Linear Regression for classification. Consider the target function:\n",
    "f(x1, x2) = sign(x2 1 + x2 2 − 0.6)\n",
    "Generate a training set of N = 1000 points on X = [−1, 1] × [−1, 1] with a uniform probability of picking each x ∈ X. Generate simulated noise by flipping the sign of\n",
    "the output in a randomly selected 10% subset of the generated training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Carry out Linear Regression without transformation, i.e., with feature vector: (1, x1, x2),to find the weight w. What is the closest value to the classification in-sample error Ein? (Run the experiment 1000 times and take the average Ein to reduce variation in your results.)  \n",
    "[a] 0 [b] 0.1 [c] 0.3 **[d] 0.5**\n",
    "[e] 0.8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_f(x):\n",
    "    if x[1]**2 + x[2]**2 -0.6 > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NonLinear_Regression(N,pic = False):\n",
    "    data = np.random.uniform(-1,1,(N,2))\n",
    "    data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "    label = np.apply_along_axis(target_f,1,data)\n",
    "    noise_point = random.sample(range(0,N),int(N*0.1))\n",
    "    label.flat[noise_point]=-label.flat[noise_point]\n",
    "    slove_LRegress(data,label)\n",
    "    if pic:\n",
    "        draw(data,label)\n",
    "    pred = np.dot(data,WLIN)\n",
    "    pred[pred>0] =1\n",
    "    pred[pred<=0] = -1\n",
    "    error = sum(pred!=label)\n",
    "    return error/float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_Nonlinear_Ein(N,times= 1000):\n",
    "    E_in = 0\n",
    "    E_in = sum(map(NonLinear_Regression,[N]*times))\n",
    "    return E_in/float(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in with noise= 0.50303\n"
     ]
    }
   ],
   "source": [
    "print \"E_in with noise=\",compute_Nonlinear_Ein(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Now, transform the N = 1000 training data into the following nonlinear feature vector:\n",
    "(1, x1, x2, x1x2, x2 1, x2 2)\n",
    "Find the vector ˜w that corresponds to the solution of Linear Regression. Which of the following hypotheses is closest to the one you find? Closest here means agrees the most with your hypothesis (has the highest probability of agreeing on a randomly selected point). Average over a few runs to make sure your answer is stable.  \n",
    "**[a] g(x1, x2) = sign(−1 − 0.05x1 + 0.08x2 + 0.13x1x2 + 1.5x2**  \n",
    "[b] g(x1, x2) = sign(−1 − 0.05x1 + 0.08x2 + 0.13x1x2 + 1.5x2  \n",
    "[c] g(x1, x2) = sign(−1 − 0.05x1 + 0.08x2 + 0.13x1x2 + 15x2  \n",
    "[d] g(x1, x2) = sign(−1 − 1.5x1 + 0.08x2 + 0.13x1x2 + 0.05x2  \n",
    "[e] g(x1, x2) = sign(−1 − 0.05x1 + 0.08x2 + 1.5x1x2 + 0.15x2\n",
    "1 + 1.5x2 1 + 15x2 1 + 1.5x2\n",
    "2)\n",
    "2) 2)\n",
    "1 + 0.05x2 1 + 0.15x2\n",
    "2)\n",
    "2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WT = np.zeros(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slove_LRegress_T(X,y):\n",
    "    global WT\n",
    "    temp = np.dot(X.T,X)\n",
    "    pseudo_inverse = np.dot(np.linalg.inv(temp),X.T)\n",
    "    WT = np.dot(pseudo_inverse,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NonLinear_Regression_T(N,pic = False):\n",
    "    data = np.random.uniform(-1,1,(N,2))\n",
    "    data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "    data = np.insert(data,3,values=data[:,1]*data[:,2],axis=1)\n",
    "    data = np.insert(data,4,values=data[:,1]**2,axis=1)\n",
    "    data = np.insert(data,5,values=data[:,2]**2,axis=1)\n",
    "    label = np.apply_along_axis(target_f,1,data)\n",
    "    noise_point = random.sample(range(0,N),int(N*0.1))\n",
    "    label.flat[noise_point]=-label.flat[noise_point]\n",
    "    slove_LRegress_T(data,label)\n",
    "    if pic:\n",
    "        draw(data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_closet(N):\n",
    "    H = np.array([[-1,-0.05,0.08,0.13,1.5,1.5],\n",
    "                  [-1,-0.05,0.08,0.13,1.5,15],\n",
    "                 [-1,-0.05,0.08,0.13,15,1.5],\n",
    "                 [-1,-1.5,0.08,0.13,0.05,0.05],\n",
    "                 [-1,-0.05,0.08,1.5,0.15,0.15]])\n",
    "    data = np.random.uniform(-1,1,(N,2))\n",
    "    data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "    data = np.insert(data,3,values=data[:,1]*data[:,2],axis=1)\n",
    "    data = np.insert(data,4,values=data[:,1]**2,axis=1)\n",
    "    data = np.insert(data,5,values=data[:,2]**2,axis=1)\n",
    "    highest = 0\n",
    "    ans = 0\n",
    "    for g in H:\n",
    "        pred_w = np.dot(data,WT)\n",
    "        pred_w[pred_w>0] = 1\n",
    "        pred_w[pred_w<=0] = -1\n",
    "        pred_g = np.dot(data,g)\n",
    "        pred_g[pred_g>0] = 1\n",
    "        pred_g[pred_g<=0] = -1\n",
    "        correct = sum(pred_w==pred_g)\n",
    "        if correct > highest:\n",
    "            highest = correct\n",
    "            ans = g\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.  , -0.05,  0.08,  1.5 ,  0.15,  0.15])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_closet(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.What is the closest value to the classification out-of-sample error Eout of your hypothesis from Problem 9? (Estimate it by generating a new set of 1000 points and adding noise, as before. Average over 1000 runs to reduce the variation in your results.)  \n",
    "[a] 0 **[b] 0.1** [c] 0.3 [d] 0.5\n",
    "[e] 0.8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_Nonlinear_Eout(N,times = 1000):\n",
    "    E_out = 0\n",
    "    for _ in range(times):\n",
    "        data = np.random.uniform(-1,1,(N,2))\n",
    "        data = np.insert(data,0,values=np.ones(N),axis=1)\n",
    "        data = np.insert(data,3,values=data[:,1]*data[:,2],axis=1)\n",
    "        data = np.insert(data,4,values=data[:,1]**2,axis=1)\n",
    "        data = np.insert(data,5,values=data[:,2]**2,axis=1)\n",
    "        label = np.apply_along_axis(target_f,1,data)\n",
    "        pred = np.dot(data,np.array([-1.  , -0.05,  0.08,  0.13,  1.5 ,  1.5 ]))\n",
    "        pred[pred>0]=1\n",
    "        pred[pred<=0]=-1\n",
    "        error = sum(pred!=label)\n",
    "        E_out += error/float(N)\n",
    "    return E_out/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_out= 0.053203\n"
     ]
    }
   ],
   "source": [
    "print \"E_out=\",compute_Nonlinear_Eout(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "cbec5ad4-453a-414a-b1b2-11b3c62dbf59",
    "theme": {
     "cbec5ad4-453a-414a-b1b2-11b3c62dbf59": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "cbec5ad4-453a-414a-b1b2-11b3c62dbf59",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
